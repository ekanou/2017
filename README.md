# TREC 2017 Core Track

### TIMETABLE
+ Collection available to participants: February, 2017
+ Topics available to participants: April 30, 2017
+ The evaluation measure 
+ Runs due from participants: May 31, 2017
+ TREC 2017 notebook paper deadline: mid-October
+ TREC 2017 conference: November 14 - 17, 2017

### Mailing list

* https://groups.google.com/d/forum/trec-core

## INTRODUCTION
The primary goals of the proposed core track are three-fold: (a) to bring together the community in a common track that could lead to a diverse set of participating runs, (b) to build one or more new test collections using more recently created documents, and (c) to establish a (new) test collection construction methodology that avoids the pitfalls of depth-k pooling.

As a side goal the track intends to:
1. study the shortcomings of test collections constructed in the past
2. experiment with new ideas for constructing test collections
3. expand existing test collections in a number of dimensions:
   * new participant tasks (ad-hoc/interactive)
   * new relevance judgments (binary/multilevel)
   * new pooling methods
   * new assessment resources (NIST / Crowd+NIST)
   * new retrieval systems contributing documents (Manual/Neural/Strong baselines)

## TRACK TASK
The participants task will be ad-hoc search. Automatic and manual runs are encouraged. The organizers of the track will provide participants with a task (title/description/narrative of TREC topics) and allow participating sites to run the experiment as they wish as long as they contribute a ranked list of documents as an output.

## DATA AND RESOURCES

**Collection**: NYTimes corpus (https://catalog.ldc.upenn.edu/ldc2008t19) â€“ Cost: $300

**Queries**: A subset of the TREC Robust queries from the Robust track.

**Relevance Assessments**:
1. NIST assessors (~50 queries)
2. Crowdsourcing (~200 queries)

## ASSESSMENT AND EVALUATION

**Evaluation**: Participating runs will be evaluated in terms of (a) their ability to rank relevant documents at the top of the returned list, and (b) their ability to contribute unique relevant documents to the pool. A suitable evaluation measure will be determined.

### Coordinators

* Evangelos Kanoulas (e.kanoulas@uva.nl), University of Amsterdam
* James Allan (allan@cs.umass.edu), University of Massachusetts
* Donna Harman (donna.harman@nist.gov), NIST 
